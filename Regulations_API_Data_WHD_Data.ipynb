{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for extracting data via API from Regulations.gov \n",
    "#### Author: Sandeep\n",
    "Sat 28 Sep 2017 12:16 AM EST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title of the Regulation**\n",
    "Application of the Fair Labor Standards Act to Domestic Service\n",
    "\n",
    "** Docket ID **\n",
    "WHD-2011-0003\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# modules imported\n",
    "import requests \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json \n",
    "import time\n",
    "import datetime as dt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# User-defined parameters change:\n",
    "\n",
    "# Defining the url (base link, API, etc)\n",
    "\n",
    "base = 'https://api.data.gov/regulations/v3/documents'\n",
    "api = 'VMXKcPf6MelG3qKCabiajcRcEoJvngQSJP3Y1Yki' # sandeep's api key\n",
    "docnum = 'WHD-2011-0003'                         # Docket Id\n",
    "dattyp = 'json'                                  # record type requested\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Some of the variables below are specific to Regulations.gov API. \n",
    "For this website, user doesn't need to enter values of 'numres' \n",
    "if the intention is collect all comments. \n",
    "Change only if you have any want test using smaller batches\n",
    "\n",
    "'''\n",
    "\n",
    "numres = 9830             # total number of records (No input required)\n",
    "rpp = 500                 # results per page (Set by the user)\n",
    "pg = 0                    # page offset (calculated in the code as per Regulations.gov instructions)\n",
    "all_limit = 1000          # request per hour allowed (Regulations.gov API default is 1000)\n",
    "nreqhr = int(numres/rpp)  # number of requests needed to download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def url_maker(base, api, docnum, dattyp, rpp, pg):\n",
    "    \n",
    "    '''Uses the parameters above (url,API key, etc.) \n",
    "           and creates url\n",
    "    '''\n",
    "    url_name = base + '.' + dattyp + '?api_key=' + api + '&D=' + docnum + '&rpp=' + str(rpp) + '&po=' + str(pg)\n",
    "    #url_name = url_base\n",
    "    \n",
    "    return url_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def request_maker(pg=0):\n",
    "    \n",
    "    '''Function makes 1 request. Outputs comments in a json file\n",
    "       Default is 0. It reuests and collects the first page of \n",
    "       results. Parameterizing for multiple requests below\n",
    "    ''' \n",
    "    res1 = requests.get(url_maker(base, api, docnum, dattyp, rpp, pg))\n",
    "    data = res1.json()    \n",
    "    \n",
    "    return (data, res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def document_info():\n",
    "    '''Collects some information regarding the Docket. Such as Total \n",
    "       number of records, etc.\n",
    "    '''\n",
    "    r = request_maker()\n",
    "    dt = r[0]\n",
    "    return (dt.get('totalNumRecords'), r[1].headers.get('X-RateLimit-Remaining'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def date_converter(date):\n",
    "    ''' Args: string date from the API\n",
    "        Output: date in datetime format '''\n",
    "    \n",
    "    t1t = date.replace(\"GMT\",\"\").rstrip()\n",
    "    t1 = dt.datetime.strptime(t1t,\"%a, %d %b %Y %H:%M:%S\")\n",
    "    #t2 - t1 + dt.timedelta(hours = 1)\n",
    "    return t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_getter():\n",
    "    \n",
    "    '''Uses the .json output file from request_maker() as input,\n",
    "       and returns pandas data frame. Tests if requests exceed the \n",
    "       alloted number per hour. Pauses the execution for some time\n",
    "    '''\n",
    "    tst = pd.DataFrame()                 # Generating an empty dataset\n",
    "    \n",
    "    \n",
    "    rate_limit = int(document_info()[1]) # starting number of available requests\n",
    "    tot_com = document_info()[0]         # total number of comments present in the docket\n",
    "    nreqhr = int(tot_com/rpp)            # total number of requests needed to extract all comments\n",
    "    \n",
    "\n",
    "    while tot_com > 0 :                  #  while total comments available or needed is positive \n",
    "        \n",
    "        new_tot_com = (rate_limit*rpp)       # apportion feasible number of comments based on the rate available\n",
    "        \n",
    "        if rate_limit > nreqhr :         # check permitted requests (if more than needed)\n",
    "            \n",
    "            print(\"inside if\")\n",
    "            \n",
    "            for val in  np.arange(0 , tot_com, rpp):\n",
    "                data = request_maker(val)\n",
    "                tmpdat = pd.DataFrame(data[0].get('documents'))\n",
    "                tst = tst.append(tmpdat)\n",
    "        \n",
    "       \n",
    "    \n",
    "            '''If the # of permitted requests available is less than needed\n",
    "            then break the API data requests into permissible chunks per hour'''\n",
    "\n",
    "        else:\n",
    "            \n",
    "            print(\"inside else\")\n",
    "            \n",
    "            start_time = int(data[1].headers.get('Date'))  # start time record\n",
    "            time1 = date_converter(start_time)             # function to convert date \n",
    "\n",
    "            \n",
    "            for val in  np.arange(0 , new_tot_com, rpp):\n",
    "                data = request_maker(val)\n",
    "                tmpdat = pd.DataFrame(data[0].get('documents'))\n",
    "                tst = tst.append(tmpdat)        \n",
    "\n",
    "            \n",
    "            \n",
    "            end_time = int(data[1].headers.get('Date'))\n",
    "            time2 = date_converter(pause_time)\n",
    "\n",
    "            halt_time = time1 + dt.timedelta(hours = 1)         # check if 1 hour has passed since\n",
    "\n",
    "            if end_time < halt_time:                            # Pause process till limit is reset at completion of the hour\n",
    "\n",
    "                seconds_wait = (half_time - pause_time).seconds # datetime objects resulting \n",
    "                time.sleep(seconds_wait.seconds())\n",
    "\n",
    "        # updating decision variables \n",
    "        tst_len = len(tst)                                     # no of comments downloaded\n",
    "        rate_limit = int(data[1].\n",
    "                         headers.get('X-RateLimit-Remaining'))  # obtain new  rate limit\n",
    "        tot_com = tot_com - tst_len                             # obtain remaining comments to extract\n",
    "        nreqhr = int(tot_com/rpp)                               # calculate requests needed to extract remaining comments\n",
    "        print(tot_com)      \n",
    "                \n",
    "    return tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dats = data_getter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dats.to_csv(docnum + \"1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"The number of comment received: {}\".format(len(dats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Unique comments: {}\".format(dats['documentId'].unique().shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next section of code is to download the attachments from comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Total Count of attachments\\n{}\".format(dats.attachmentCount.value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tt1 = dats.loc[dats['attachmentCount']>0,['documentId','attachmentCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Over `5100` attachments in this docket.  \n",
    "To avoid getting tied up in this subprogram  \n",
    "I am commented it out.   \n",
    "Uncomment the below code if you want to run this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "url2 = 'https://www.regulations.gov/contentStreamer?documentId={}&attachmentNumber={}&contentType=pdf'\n",
    "\n",
    "def doc_extract():\n",
    "    \n",
    "         \n",
    "        #Extracts attachments from comments that using \n",
    "        #the pandas DataFrame to filter comments with \n",
    "        #attachments. Saves them in the current \n",
    "        #working directory.\n",
    "        \n",
    "    \n",
    "    for row in tt1.index:\n",
    "        id1 = tt1.loc[row,'documentId']\n",
    "        at2 = tt1.loc[row, 'attachmentCount']\n",
    "        \n",
    "        for i in np.arange(1,at2+1):\n",
    "            \n",
    "            newurl = url2.format(str(id1),str(i))\n",
    "            \n",
    "            r = rq.get(newurl)\n",
    "            \n",
    "            fnm = 'att{}-{}.pdf'.format(str(id1),str(i))\n",
    "                \n",
    "            # alternate filename\n",
    "            #r.headers.get('Content-Disposition').split('=')[1]\n",
    "               \n",
    "            with open(fnm, 'wb') as fd:\n",
    "                for chunk in r.iter_content(chunk_size=128):\n",
    "                    fd.write(chunk)\n",
    "                    \n",
    "    return None\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
